{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_CHATBOT_TEST_KEY_INTERNAL\"]\n",
    "MONGO_URI = os.environ[\"MONGO_URI\"]\n",
    "EMBEDDING_MODEL_NAME = os.environ[\"EMBEDDING_MODEL_NAME\"]\n",
    "EMBEDDING_DIMENSIONS = os.environ[\"EMBEDDING_DIMENSIONS\"]\n",
    "CHAT_MODEL_NAME = os.environ[\"CHAT_MODEL_NAME\"]\n",
    "\n",
    "DB_NAME = \"gaia\"\n",
    "COLLECTION_NAME = \"documents\"\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\"\n",
    "MAX_CHUNKS_TO_RETRIEVE=10\n",
    "CHUNK_MIN_RELEVANCE_SCORE=0.2\n",
    "\n",
    "MAX_TOKENS_FOR_RESPONSE = 500\n",
    "CHAT_MODEL_TEMPERATURE=0.3\n",
    "CHAT_MODEL_FREQ_PENALTY=0\n",
    "CHAT_MODEL_PRES_PENALTY=0\n",
    "MEMORY_WINDOW_SIZE = 5  # Number of turns to keep in memory before summarizing\n",
    "SHOW_VERBOSE=True\n",
    "\n",
    "\n",
    "PARENT_PATH = Path.cwd().parent\n",
    "EVA_SETTINGS_PATH = PARENT_PATH / 'evasettings'\n",
    "EVA_SETTINGS_ENVIRONMENT_DIRECTORY = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the project root by navigating two levels up from the current notebook directory\n",
    "project_root = Path.cwd().parent.parent\n",
    "\n",
    "# Define paths to models and vectordatabases directories relative to the project root\n",
    "models_path = project_root / 'scripts' / 'models'\n",
    "vectordatabases_path = project_root / 'scripts' / 'vectordatabases'\n",
    "\n",
    "# Add the paths to sys.path if they're not already in it\n",
    "if str(models_path) not in sys.path:\n",
    "    sys.path.append(str(models_path))\n",
    "if str(vectordatabases_path) not in sys.path:\n",
    "    sys.path.append(str(vectordatabases_path))\n",
    "\n",
    "# Print sys.path for debugging\n",
    "print(f\"Models Path: {models_path}\")\n",
    "print(f\"Vector Databases Path: {vectordatabases_path}\")\n",
    "print(sys.path)\n",
    "\n",
    "# Try importing the modules now\n",
    "from models import model_rag\n",
    "from vectordatabases import BaseDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, chat_data):\n",
    "        self.chat_data = chat_data\n",
    "\n",
    "    ## Public Methods\n",
    "    def get_response(self):\n",
    "        self.llm_eva = ChatOpenAI(\n",
    "            model_name=self.chat_data.rag_settings.chat_model_name,\n",
    "            temperature=self.chat_data.rag_settings.temperature,\n",
    "            max_tokens=self.chat_data.rag_settings.max_tokens_for_response,\n",
    "            openai_api_key=self.chat_data.llm_settings.llm_key\n",
    "        )\n",
    "\n",
    "        self.summarized_history = \"\"\n",
    "        self.memory = None\n",
    "        if self.chat_data.chat_history:\n",
    "            self.summarized_history, self.memory = self._summarize_history()\n",
    "        \n",
    "        # Detect the intent first\n",
    "        intent_name = self._detect_intent()\n",
    "        print(\"Detected Intent: \", intent_name)\n",
    "\n",
    "        # Now invoke the QA model to get the response\n",
    "        qa = self._get_qa_instance(intent_name)\n",
    "        result = qa.invoke({\"question\": self.chat_data.user_input})\n",
    "\n",
    "        response_text = result.get(\"answer\", \"\")\n",
    "        sources_list = result.get(\"sources\", \"\")\n",
    "\n",
    "        # return response_text, sources_list\n",
    "    \n",
    "        return model_rag.ChatResponse(response=response_text, sources=sources_list)\n",
    "\n",
    "    ## Private Methods\n",
    "    def _load_template(self, project_template_directory_name, template_file_name):\n",
    "        project_template_directory_path = os.path.join(EVA_SETTINGS_PATH, project_template_directory_name, EVA_SETTINGS_ENVIRONMENT_DIRECTORY)\n",
    "        template_file_path = project_template_directory_path+ '/' + template_file_name\n",
    "        with open(template_file_path, \"r\") as file:\n",
    "            return file.read()\n",
    "\n",
    "    def _build_intent_detection_prompt(self):\n",
    "        # Load intent detection template\n",
    "        intent_detection_template = self._load_template(\n",
    "            self.chat_data.prompt_template_directory_name, \n",
    "            self.chat_data.intent_detection_prompt_template_file_name\n",
    "        )\n",
    "        \n",
    "        # Dynamically build the intent list from intent details\n",
    "        intent_list = \"\\n\".join(\n",
    "            [f'- \"{intent_name}\": {intent_data.description}' for intent_name, intent_data in self.chat_data.intent_details].items()]\n",
    "        )\n",
    "        \n",
    "        # Build the full prompt for intent detection\n",
    "        prompt = intent_detection_template.format(\n",
    "            user_input=self.chat_data.user_input,\n",
    "            history=self.summarized_history,  # Use summarized history here\n",
    "            intent_list=intent_list\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "    def _build_chat_prompt(self, intent_name):\n",
    "        # Load base template\n",
    "        base_template = self._load_template(self.chat_data.prompt_template_directory_name, self.chat_data.base_prompt_template_file_name])\n",
    "        \n",
    "        # Load intent-specific template or default to generic message\n",
    "        if intent_name == \"none\":\n",
    "            intent_template = \"\"\n",
    "        else:\n",
    "            intent_filename = self.chat_dataintent_details.get(intent_name).filename\n",
    "            intent_template = self._load_template(self.chat_data.prompt_template_directory_name, intent_filename)\n",
    "        \n",
    "        # Build the full prompt using the base and intent templates\n",
    "        return base_template.format(\n",
    "            subinstructions=intent_template,\n",
    "            history=self.summarized_history,  # Pass the summarized history for context\n",
    "            summaries=\"{summaries}\",\n",
    "            question=\"{question}\"\n",
    "        )\n",
    "\n",
    "    def _get_qa_retriever(self):\n",
    "        llm_embeddings = OpenAIEmbeddings(\n",
    "            model=self.chat_data.llm_settings.embedding_model_name,\n",
    "            openai_api_key=self.chat_data.llm_settings.llm_key\n",
    "        )\n",
    "    \n",
    "        db_instance = BaseDB().get_vector_db(\n",
    "            self.chat_data.db_type,\n",
    "            self.chat_data.db_settings,\n",
    "            llm_embeddings\n",
    "        )\n",
    "        vector_store = db_instance.vector_index\n",
    "    \n",
    "        qa_retriever = vector_store.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"k\": self.chat_data.rag_settings.max_chunks_to_retrieve.value, \"score_threshold\": self.chat_data.rag_settings.retrieved_chunks_min_relevance_score.value})\n",
    "        \n",
    "        return qa_retriever\n",
    "    \n",
    "        \n",
    "    def _summarize_history(self):\n",
    "        if not self.chat_data.chat_history:\n",
    "            return \"\", None\n",
    "        \n",
    "        history = ChatMessageHistory()\n",
    "        for conv in self.chat_data.chat_history:\n",
    "            history.add_user_message(conv.human)\n",
    "            history.add_ai_message(conv.ai)\n",
    "            \n",
    "        memory = ConversationSummaryMemory.from_messages(\n",
    "            llm=self.llm_eva,\n",
    "            chat_memory=history,\n",
    "            return_messages=True,\n",
    "            memory_key=\"history\",\n",
    "            input_key=\"question\"\n",
    "        )\n",
    "\n",
    "        # Get the summarized history\n",
    "        #summarized_history = memory.load_memory_variables({}).get('history', '')\n",
    "        summarized_history = memory.buffer\n",
    "\n",
    "        print('summarized_history:', summarized_history)\n",
    "       \n",
    "        return summarized_history, memory\n",
    "\n",
    "    def _get_qa_instance(self, intent_name):\n",
    "        dynamic_prompt_content = self._build_chat_prompt(intent_name)\n",
    "            \n",
    "        prompt_template = PromptTemplate(\n",
    "            template=dynamic_prompt_content,\n",
    "            input_variables=['summaries', 'question']\n",
    "        )\n",
    "    \n",
    "        qa_retriever = self._get_qa_retriever()\n",
    "        \n",
    "        qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "            llm=self.llm_eva,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=qa_retriever,\n",
    "            return_source_documents=False,\n",
    "            chain_type_kwargs={\n",
    "                \"verbose\": SHOW_VERBOSE,\n",
    "                \"prompt\": prompt_template,\n",
    "                \"memory\": self.memory  # Include memory if available\n",
    "            } if self.memory else {\n",
    "                \"verbose\": SHOW_VERBOSE,\n",
    "                \"prompt\": prompt_template\n",
    "            }\n",
    "        )\n",
    "    \n",
    "        return qa\n",
    "\n",
    "    def _detect_intent(self):\n",
    "        intent_prompt = self._build_intent_detection_prompt()\n",
    "        \n",
    "        prompt_template = PromptTemplate(\n",
    "            template=intent_prompt,\n",
    "            input_variables=[\"user_input\", \"history\", \"intent_list\"]\n",
    "        )\n",
    "        \n",
    "        intent_chain = RunnableSequence(prompt_template, self.llm_eva)\n",
    "        intent_result = intent_chain.invoke({\n",
    "            \"user_input\": self.chat_data.user_input,  \n",
    "            \"history\": self.summarized_history,  \n",
    "            \"intent_list\": \"\\n\".join([f'- \"{intent_name}\"' for intent_name in self.chat_data.intent_details.keys()])\n",
    "        })\n",
    "\n",
    "        detected_intent = intent_result.content.strip()\n",
    "        if detected_intent not in self.chat_data.intent_details:\n",
    "            return \"none\"\n",
    "        return detected_intent\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "\n",
    "def get_chatbot_response(payload: model_rag.ChatRequest):\n",
    "    try:\n",
    "        chat_processor = rag.RAG(payload)\n",
    "        response = chat_processor.get_response()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Chatbot request failed: {str(e)}\")\n",
    "\n",
    "\n",
    "def call_chatbot_endpoint(user_input_text):\n",
    "    global conversation_history\n",
    "    \n",
    "    data = {\n",
    "        \"db_type\": \"mongodb\",\n",
    "        \"db_settings\": {\n",
    "            \"uri\": MONGO_URI,  \n",
    "            \"db_name\": DB_NAME,  \n",
    "            \"collection_name\": COLLECTION_NAME,  \n",
    "            \"vector_index_name\": ATLAS_VECTOR_SEARCH_INDEX_NAME,  \n",
    "            \"vector_similarity_function\": \"cosine\"  \n",
    "        },\n",
    "        \"llm_settings\": {\n",
    "            \"llm_key\": OPENAI_API_KEY,  \n",
    "            \"vector_dimension_size\": EMBEDDING_DIMENSIONS,  \n",
    "            \"embedding_model_name\": EMBEDDING_MODEL_NAME  \n",
    "        },\n",
    "        \"rag_settings\": {\n",
    "            \"chat_model_name\": CHAT_MODEL_NAME,  \n",
    "            \"max_chunks_to_retrieve\": MAX_CHUNKS_TO_RETRIEVE,  \n",
    "            \"retrieved_chunks_min_relevance_score\": CHUNK_MIN_RELEVANCE_SCORE,\n",
    "            \"max_tokens_for_response\": MAX_TOKENS_FOR_RESPONSE,  \n",
    "            \"temperature\": CHAT_MODEL_TEMPERATURE,  \n",
    "            \"frequency_penalty\": CHAT_MODEL_FREQ_PENALTY,\n",
    "            \"presence_penalty\": CHAT_MODEL_PRES_PENALTY\n",
    "        },\n",
    "        \"user_input\": user_input_text,\n",
    "        \"chat_history\": conversation_history,\n",
    "        \"prompt_template_directory_name\": \"gaia\",  \n",
    "        \"base_prompt_template_file_name\": \"base_template.txt\", \n",
    "        \"intent_detection_prompt_template_file_name\": \"detect_intent.txt\", \n",
    "        \"intent_details\": {\n",
    "            \"diagnosis\": {\n",
    "                \"filename\": \"diagnosis.txt\",\n",
    "                \"description\": \"This intent covers diagnosis-related queries for crop issues.\"\n",
    "            },\n",
    "            \"symptoms_identification\": {\n",
    "                \"filename\": \"symptoms_identification.txt\",\n",
    "                \"description\": \"This intent helps identify symptoms for a specific pest or problem.\"\n",
    "            },\n",
    "            \"pest_list\": {\n",
    "                \"filename\": \"pest_list.txt\",\n",
    "                \"description\": \"This intent provides a list of pests affecting specific crops in a given location.\"\n",
    "            },\n",
    "            \"ipm_pest_management\": {\n",
    "                \"filename\": \"ipm_pest_management.txt\",\n",
    "                \"description\": \"This intent provides integrated pest management advice, including biocontrol and chemical recommendations.\"\n",
    "            },\n",
    "            \"chemical_handling_safety\": {\n",
    "                \"filename\": \"chemical_handling_safety.txt\",\n",
    "                \"description\": \"This intent provides safety advice regarding the handling and application of chemicals.\"\n",
    "            },\n",
    "            \"invasive_pest_status\": {\n",
    "                \"filename\": \"invasive_pest_status.txt\",\n",
    "                \"description\": \"This intent provides the current status of invasive pests in a specific region.\"\n",
    "            },\n",
    "            \"dosage_recommendations\": {\n",
    "                \"filename\": \"dosage_recommendations.txt\",\n",
    "                \"description\": \"This intent provides dosage recommendations for specific chemical or biocontrol products.\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # chat_processor = RAG(data)\n",
    "    # response_text, sources_list = chat_processor.get_response()\n",
    "    \n",
    "    response = get_chatbot_response(payload=data)\n",
    "    \n",
    "    conversation_history.append({\n",
    "        \"human\": user_input_text,  \n",
    "        \"ai\": response.response_text  \n",
    "    })\n",
    "    \n",
    "    return response.response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_user_input(\"What is fall army worm?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle_user_input(\"Which regions fall army worm affect?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle_user_input(\"Hi, what are pests that affect tomatoes in Jamaica?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle_user_input(\"And what are other pest affecting that region?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle_user_input(\"I didn't mentiond rice but answered for rice. why?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
