{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_CHATBOT_TEST_KEY_INTERNAL\"]\n",
    "MONGO_URI = os.environ[\"MONGO_URI\"]\n",
    "EMBEDDING_MODEL_NAME = os.environ[\"EMBEDDING_MODEL_NAME\"]\n",
    "EMBEDDING_DIMENSIONS = os.environ[\"EMBEDDING_DIMENSIONS\"]\n",
    "CHAT_MODEL_NAME = os.environ[\"CHAT_MODEL_NAME\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"OPENAI_API_CHATBOT_TEST_KEY_INTERNAL\"]\n",
    "\n",
    "DB_NAME = \"gaia\"\n",
    "COLLECTION_NAME = \"documents\"\n",
    "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\"\n",
    "MAX_CHUNKS_TO_RETRIEVE=10\n",
    "CHUNK_MIN_RELEVANCE_SCORE=0.4\n",
    "\n",
    "MAX_TOKENS_FOR_RESPONSE = 1000\n",
    "CHAT_MODEL_TEMPERATURE=0\n",
    "CHAT_MODEL_FREQ_PENALTY=0.2\n",
    "CHAT_MODEL_PRES_PENALTY=0.2\n",
    "SHOW_VERBOSE=False\n",
    "\n",
    "\n",
    "PARENT_PATH = Path.cwd().parent\n",
    "EVA_SETTINGS_PATH = PARENT_PATH / 'evasettings'\n",
    "EVA_SETTINGS_ENVIRONMENT_DIRECTORY = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = PARENT_PATH / 'scripts' / 'models'\n",
    "vectordatabases_path = PARENT_PATH / 'scripts' / 'vectordatabases'\n",
    "\n",
    "if str(models_path) not in sys.path:\n",
    "    sys.path.append(str(models_path))\n",
    "if str(vectordatabases_path) not in sys.path:\n",
    "    sys.path.append(str(vectordatabases_path))\n",
    "\n",
    "from models import model_rag\n",
    "from vectordatabases import BaseDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ragas import evaluate\n",
    "# from datasets import Dataset\n",
    "# from ragas.metrics import (\n",
    "#     answer_relevancy,\n",
    "#     answer_correctness\n",
    "# )\n",
    "\n",
    "# metrics = [\n",
    "#     answer_relevancy,\n",
    "#     answer_correctness\n",
    "# ]\n",
    "\n",
    "# retrieved_contexts = [doc.page_content for doc in sources_documents]\n",
    "# reference = \" \".join(retrieved_contexts) if retrieved_contexts else \"\"\n",
    "# response_dataset = [\n",
    "#     {\n",
    "#         \"question\": f\"{self.summarized_history}\\n{self.chat_data.user_input}\",\n",
    "#         \"answer\": response_text,\n",
    "#         \"retrieved_contexts\": retrieved_contexts, \n",
    "#         \"reference\": reference \n",
    "#     }\n",
    "# ]\n",
    "# response_scores = self._run_ragas_evaluation(response_dataset)\n",
    "# print(\"Generated Response Evaluation Scores: \", response_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, chat_data):\n",
    "        self.chat_data = chat_data\n",
    "\n",
    "    ## Public Methods\n",
    "    def get_response(self):\n",
    "        self.llm_eva = ChatOpenAI(\n",
    "            model_name=self.chat_data.rag_settings.chat_model_name,\n",
    "            temperature=self.chat_data.rag_settings.temperature,\n",
    "            max_tokens=self.chat_data.rag_settings.max_tokens_for_response,\n",
    "            openai_api_key=self.chat_data.llm_settings.llm_key\n",
    "        )\n",
    "\n",
    "        self.summarized_history = \"\"\n",
    "        self.memory = None\n",
    "        if self.chat_data.chat_history:\n",
    "            self.summarized_history, self.memory = self._summarize_history()\n",
    "        \n",
    "        # Detect the intent first\n",
    "        intent_name = self._detect_intent()\n",
    "        print(\"Detected Intent: \", intent_name)\n",
    "        print()\n",
    "\n",
    "        # Now invoke the QA model to get the response\n",
    "        qa = self._get_qa_instance(intent_name)\n",
    "        result = qa.invoke({\"question\": self.chat_data.user_input})\n",
    "        \n",
    "        response_text = result.get(\"answer\", \"\")\n",
    "        response_text, display_source = self._format_response(response_text)\n",
    "        if display_source is True:\n",
    "            sources_documents = result.get(\"source_documents\", [])            \n",
    "            sources_list = self._extract_sources(sources_documents)\n",
    "        else:\n",
    "            sources_list=[]\n",
    "        \n",
    "        return model_rag.ChatResponse(response=response_text, sources=sources_list)\n",
    "\n",
    "        \n",
    "    ## Private Methods\n",
    "\n",
    "    def _extract_sources(self, sources_documents):\n",
    "        sources_list = []\n",
    "        for doc in sources_documents:\n",
    "            language=doc.metadata.get(next((key for key in doc.metadata if key.lower() == \"language\"), \"\"), \"\")\n",
    "            if language.lower()==\"english\":\n",
    "                sources_list.append(\n",
    "                    model_rag.Source(\n",
    "                        source=doc.metadata.get(next((key for key in doc.metadata if key.lower() == \"source\"), \"\"), \"\"),\n",
    "                        type=doc.metadata.get(next((key for key in doc.metadata if key.lower() == \"type\"), \"\"), \"\"),\n",
    "                        title=doc.metadata.get(next((key for key in doc.metadata if key.lower() == \"title\"), \"\"), \"\"),\n",
    "                        country=doc.metadata.get(next((key for key in doc.metadata if key.lower() == \"country\"), \"\"), \"\"),\n",
    "                        language=doc.metadata.get(next((key for key in doc.metadata if key.lower() == \"language\"), \"\"), \"\")\n",
    "                    )\n",
    "                )\n",
    "        return sources_list\n",
    "\n",
    "    def _format_response(self, response_text):\n",
    "        display_source = False\n",
    "        try:\n",
    "            if \"<displaysource>\" in response_text and \"</displaysource>\" in response_text:\n",
    "                display_source = response_text.split(\"<displaysource>\")[1].split(\"</displaysource>\")[0].strip() == \"true\"\n",
    "                response_text = response_text.replace(f\"<displaysource>{'true' if display_source else 'false'}</displaysource>\", \"\")\n",
    "        except Exception as e:\n",
    "            pass \n",
    "    \n",
    "        return response_text, display_source\n",
    "        \n",
    "    \n",
    "    def _load_template(self, project_template_directory_name, template_file_name):\n",
    "        project_template_directory_path = os.path.join(EVA_SETTINGS_PATH, project_template_directory_name, EVA_SETTINGS_ENVIRONMENT_DIRECTORY)\n",
    "        template_file_path = project_template_directory_path + '/' + template_file_name\n",
    "        with open(template_file_path, \"r\") as file:\n",
    "            return file.read()\n",
    "\n",
    "    def _build_intent_detection_prompt(self):\n",
    "        # Load intent detection template\n",
    "        intent_detection_template = self._load_template(\n",
    "            self.chat_data.prompt_template_directory_name, \n",
    "            self.chat_data.intent_detection_prompt_template_file_name\n",
    "        )\n",
    "        \n",
    "        # Dynamically build the intent list from intent details\n",
    "        intent_list = \"\\n\".join(\n",
    "            [f'- \"{intent_name}\": {intent_data.description}' for intent_name, intent_data in self.chat_data.intent_details.items()]\n",
    "        )\n",
    "        \n",
    "        # Build the full prompt for intent detection\n",
    "        prompt = intent_detection_template.format(\n",
    "            user_input=self.chat_data.user_input,\n",
    "            history=self.summarized_history,  # Use summarized history here\n",
    "            intent_list=intent_list\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "    def _build_chat_prompt(self, intent_name):\n",
    "        # Load base template\n",
    "        base_template = self._load_template(\n",
    "            self.chat_data.prompt_template_directory_name, \n",
    "            self.chat_data.base_prompt_template_file_name\n",
    "        )\n",
    "        \n",
    "        # Load intent-specific template or default to generic message\n",
    "        if intent_name == \"none\":\n",
    "            intent_template = \"\"\n",
    "        else:\n",
    "            intent_filename = self.chat_data.intent_details.get(intent_name).filename\n",
    "            intent_template = self._load_template(self.chat_data.prompt_template_directory_name, intent_filename)\n",
    "        \n",
    "        # Build the full prompt using the base and intent templates\n",
    "        return base_template.format(\n",
    "            subinstructions=intent_template,\n",
    "            history=self.summarized_history,  # Pass the summarized history for context\n",
    "            summaries=\"{summaries}\",\n",
    "            question=\"{question}\"\n",
    "        )\n",
    "\n",
    "    def _get_qa_retriever(self):\n",
    "        llm_embeddings = OpenAIEmbeddings(\n",
    "            model=self.chat_data.llm_settings.embedding_model_name,\n",
    "            openai_api_key=self.chat_data.llm_settings.llm_key\n",
    "        )\n",
    "    \n",
    "        db_instance = BaseDB().get_vector_db(\n",
    "            self.chat_data.db_type,\n",
    "            self.chat_data.db_settings,\n",
    "            llm_embeddings\n",
    "        )\n",
    "        vector_store = db_instance.vector_index\n",
    "    \n",
    "        qa_retriever = vector_store.as_retriever(\n",
    "            # search_type=\"similarity_score_threshold\", \n",
    "            # search_kwargs={\n",
    "            #     \"k\": self.chat_data.rag_settings.max_chunks_to_retrieve.value, \n",
    "            #     \"score_threshold\": self.chat_data.rag_settings.retrieved_chunks_min_relevance_score.value\n",
    "            # }\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": self.chat_data.rag_settings.max_chunks_to_retrieve.value, \n",
    "                           \"fetch_k\": 50, \n",
    "                           \"lambda_mult\": 0.5\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return qa_retriever\n",
    "\n",
    "\n",
    "    def _get_qa_instance(self, intent_name):\n",
    "        dynamic_prompt_content = self._build_chat_prompt(intent_name)\n",
    "            \n",
    "        prompt_template = PromptTemplate(\n",
    "            template=dynamic_prompt_content,\n",
    "            input_variables=['summaries', 'question']\n",
    "        )\n",
    "    \n",
    "        qa_retriever = self._get_qa_retriever()\n",
    "\n",
    "        if self.memory:\n",
    "            chain_type_kwargs = {\n",
    "                \"verbose\": SHOW_VERBOSE,\n",
    "                \"prompt\": prompt_template,\n",
    "                \"memory\": self.memory  # Include memory if available\n",
    "            }\n",
    "        else:\n",
    "            chain_type_kwargs = {\n",
    "                \"verbose\": SHOW_VERBOSE,\n",
    "                \"prompt\": prompt_template\n",
    "            }\n",
    "\n",
    "        qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "            llm=self.llm_eva,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=qa_retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs=chain_type_kwargs\n",
    "        )\n",
    "    \n",
    "        return qa\n",
    "\n",
    "\n",
    "    def _detect_intent(self):\n",
    "        intent_prompt = self._build_intent_detection_prompt()\n",
    "        \n",
    "        prompt_template = PromptTemplate(\n",
    "            template=intent_prompt,\n",
    "            input_variables=[\"user_input\", \"history\", \"intent_list\"]\n",
    "        )\n",
    "        \n",
    "        intent_chain = RunnableSequence(prompt_template, self.llm_eva)\n",
    "        intent_result = intent_chain.invoke({\n",
    "            \"user_input\": self.chat_data.user_input,  \n",
    "            \"history\": self.summarized_history,  \n",
    "            \"intent_list\": \"\\n\".join([f'- \"{intent_name}\"' for intent_name in self.chat_data.intent_details.keys()])\n",
    "        })\n",
    "\n",
    "        detected_intent = intent_result.content.strip().strip(' \"\\'').lower()\n",
    "        if detected_intent not in [intent_name.lower() for intent_name in self.chat_data.intent_details.keys()]:\n",
    "            return \"none\"\n",
    "        return detected_intent\n",
    "\n",
    "    def _summarize_history(self):\n",
    "        if not self.chat_data.chat_history:\n",
    "            return \"\", None\n",
    "        \n",
    "        history = ChatMessageHistory()\n",
    "        for conv in self.chat_data.chat_history:\n",
    "            if conv.role.lower() == 'human':\n",
    "                history.add_message(HumanMessage(content=conv.message))\n",
    "            elif conv.role.lower() == 'ai':\n",
    "                history.add_message(SystemMessage(content=conv.message))\n",
    "            \n",
    "        memory = ConversationSummaryMemory.from_messages(\n",
    "            llm=self.llm_eva,\n",
    "            chat_memory=history,\n",
    "            return_messages=True,\n",
    "            memory_key=\"history\",\n",
    "            input_key=\"question\"\n",
    "        )\n",
    "\n",
    "        summarized_history = memory.buffer\n",
    "        return summarized_history, memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "\n",
    "def get_chatbot_response(payload: model_rag.ChatRequest):\n",
    "    chat_processor = RAG(payload)\n",
    "    response = chat_processor.get_response()\n",
    "    return response\n",
    "   \n",
    "\n",
    "\n",
    "def call_chatbot_endpoint(user_input_text):\n",
    "    global conversation_history\n",
    "    \n",
    "    # Directly create an instance of model_rag.ChatRequest with the required values\n",
    "    chat_request = model_rag.ChatRequest(\n",
    "        db_type=\"mongodb\",\n",
    "        db_settings={\n",
    "            \"uri\": MONGO_URI,  \n",
    "            \"db_name\": DB_NAME,  \n",
    "            \"collection_name\": COLLECTION_NAME,  \n",
    "            \"vector_index_name\": ATLAS_VECTOR_SEARCH_INDEX_NAME,  \n",
    "            \"vector_similarity_function\": \"cosine\"\n",
    "        },\n",
    "        llm_settings={\n",
    "            \"llm_key\": OPENAI_API_KEY,  \n",
    "            \"vector_dimension_size\": EMBEDDING_DIMENSIONS,  \n",
    "            \"embedding_model_name\": EMBEDDING_MODEL_NAME\n",
    "        },\n",
    "        rag_settings={\n",
    "            \"chat_model_name\": CHAT_MODEL_NAME,  \n",
    "            \"max_chunks_to_retrieve\": MAX_CHUNKS_TO_RETRIEVE,  \n",
    "            \"retrieved_chunks_min_relevance_score\": CHUNK_MIN_RELEVANCE_SCORE,\n",
    "            \"max_tokens_for_response\": MAX_TOKENS_FOR_RESPONSE,  \n",
    "            \"temperature\": CHAT_MODEL_TEMPERATURE,  \n",
    "            \"frequency_penalty\": CHAT_MODEL_FREQ_PENALTY,\n",
    "            \"presence_penalty\": CHAT_MODEL_PRES_PENALTY\n",
    "        },\n",
    "        user_input=user_input_text,\n",
    "        chat_history=conversation_history,\n",
    "        prompt_template_directory_name=\"gaia\",  \n",
    "        base_prompt_template_file_name=\"base_template.txt\", \n",
    "        intent_detection_prompt_template_file_name=\"detect_intent.txt\", \n",
    "        intent_details = {\n",
    "            \"diagnosis\": {\n",
    "                \"filename\": \"diagnosis.txt\",\n",
    "                \"description\": \"This intent covers queries related to diagnosing pests or problems affecting crops, including identifying potential pests or diseases based on symptoms, crop type, and location.\"\n",
    "            },\n",
    "            \"symptoms_identification\": {\n",
    "                \"filename\": \"symptoms_identification.txt\",\n",
    "                \"description\": \"This intent provides detailed information about symptoms caused by a specific pest or problem, including visual indicators and progression of the symptoms.\"\n",
    "            },\n",
    "            \"pest_list\": {\n",
    "                \"filename\": \"pest_list.txt\",\n",
    "                \"description\": \"This intent provides a list of pests that affect a specific crop in a specific country or region.\"\n",
    "            },\n",
    "            \"ipm_pest_management\": {\n",
    "                \"filename\": \"ipm_pest_management.txt\",\n",
    "                \"description\": \"This intent provides integrated pest management (IPM) advice, including prevention strategies, biocontrol recommendations, and chemical pesticide usage for managing pests or diseases on crops.\"\n",
    "            },\n",
    "            \"chemical_handling_safety\": {\n",
    "                \"filename\": \"chemical_handling_safety.txt\",\n",
    "                \"description\": \"This intent provides safety advice for handling and applying specific chemicals, including personal protective equipment (PPE), safe storage, and disposal recommendations.\"\n",
    "            },\n",
    "            \"invasive_pest_status\": {\n",
    "                \"filename\": \"invasive_pest_status.txt\",\n",
    "                \"description\": \"This intent provides information on the current status, distribution, and spread of invasive pests in a specific country or region.\"\n",
    "            },\n",
    "            \"dosage_recommendations\": {\n",
    "                \"filename\": \"dosage_recommendations.txt\",\n",
    "                \"description\": \"This intent provides dosage recommendations for chemical or biocontrol products, including application rates, frequency, and any location-specific restrictions or precautions.\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    )\n",
    "    \n",
    "    chatbot_response = get_chatbot_response(payload=chat_request)\n",
    "    \n",
    "    conversation_history.append({\n",
    "        \"role\": \"Human\",\n",
    "        \"message\": user_input_text,  \n",
    "    })\n",
    "    conversation_history.append({\n",
    "        \"role\": \"AI\",  \n",
    "        \"message\": chatbot_response.response  \n",
    "    })\n",
    "\n",
    "\n",
    "    print('Bot''s Response:', chatbot_response.response)\n",
    "    print('Sources: ', chatbot_response.sources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_chatbot_endpoint(\"sdasd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Intent:  diagnosis\n",
      "\n",
      "Bots Response: Sure, I can help with that! \n",
      "\n",
      "I understand that you're dealing with coffee in Kenya. Could you please provide more details about the symptoms you're observing and which part of the plant is affected? This will help me give you a more accurate diagnosis and recommendations. \n",
      "\n",
      "For example:\n",
      "- Are you noticing any discoloration, wilting, or unusual spots on the leaves?\n",
      "- Is the issue affecting the berries, leaves, stems, or roots?\n",
      "\n",
      "Once I have more information, I can assist you better.\n",
      "Sources:  []\n"
     ]
    }
   ],
   "source": [
    "call_chatbot_endpoint(\"Hi, I need help to diagnose a problem related to coffee in Kenya?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Intent:  diagnosis\n",
      "\n",
      "Bots Response: I understand that you're dealing with coffee in Kenya, with symptoms such as yellowing and wilting leaves affecting the leaves.\n",
      "\n",
      "Based on the information provided, it is possible that your coffee plants are suffering from Coffee Wilt Disease. Here are some details about this disease:\n",
      "\n",
      "- **Cause**: Coffee Wilt Disease is caused by a fungus that lives in the soil and dead infected plant materials. The fungus enters the plant through wounds on the stem and roots.\n",
      "- **Symptoms**: The fungus grows inside the plant and blocks the movement of water up the plant, leading to wilting leaves.\n",
      "- **Management**: Unfortunately, there is no chemical control for Coffee Wilt Disease once the plant is infected. Preventative measures include:\n",
      "  - Avoiding injuries to the plants that can serve as entry points for the fungus.\n",
      "  - Removing and destroying infected plant materials to reduce the spread of the fungus.\n",
      "  - Implementing good sanitation practices in the field.\n",
      "\n",
      "For more detailed and specific advice, you may want to consult with your local extension service or visit the PlantwisePlus Knowledge Bank.\n",
      "\n",
      "\n",
      "Sources:  [Source(source='20117800551', type='PFFF', title='Maize streak virus', country='Kenya', language='English'), Source(source='20157800304', type='PMDG', title='Nitrogen deficiency in Maize', country='Kenya', language='English'), Source(source='20117800547', type='PFFF', title='Preventing coffee wilt disease', country='Kenya', language='English'), Source(source='20187800648', type='PMDG', title='Stem borer on Rice', country='India', language='English'), Source(source='20137803905', type='PFFF', title='Yellow stem borer on paddy', country='India', language='English'), Source(source='20137804242', type='PFFF', title='Whiteflies', country=\"['Kenya', 'India']\", language='English'), Source(source='20167800567', type='PFFF', title='Alectra vogelii', country=\"['Kenya', 'India']\", language='English'), Source(source='20167800635', type='PMDG', title='Cercospora leaf spot of maize', country='Kenya', language='English'), Source(source='20157800012', type='PFFF', title='False smut Disease of Paddy', country='India', language='English'), Source(source='25059702', type='Countrywise-Biocontrol-Use', title='Biostimul', country='India', language='English')]\n"
     ]
    }
   ],
   "source": [
    "call_chatbot_endpoint(\"It is affecting the leaves. Yellow and wilting leaves.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
